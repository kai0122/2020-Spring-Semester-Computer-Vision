{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV_Group_Assignment_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEEzMrOoujQN",
        "colab_type": "text"
      },
      "source": [
        "# Group Assignment Computer Vision\n",
        "\n",
        "Authors:\n",
        "- Robbe Neyns\n",
        "- Tingyu Qu\n",
        "- Emiel Vandeloo\n",
        "- Po-Kai Yang"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tADayWWc7k2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install opencv-python\n",
        "!pip install opencv-contrib-python==3.4.2.17\n",
        "!pip install -q keras_vggface\n",
        "!pip install -q tensorflow==1.14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p31dA5CUvzFT",
        "colab_type": "text"
      },
      "source": [
        "This notebook explores a couple of feature representations for the task of face recognition. Two types of representations are being tested: hand-crafted features and data-driven features. Scale Invariant Feature Transform (SIFT) features are used as an example of the first, while Principal Component Analysis (PCA) and a pre-trained deep learning model are leveraged as examples of the latter. We use, evaluate and compare these methods in the context of classifying and identifying faces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcibx-g0yc-R",
        "colab_type": "text"
      },
      "source": [
        "First, we build a dataset of face images. The chosen people are Ang Lee, Natalie Portman, Jack Ma and Miranda Kerr. Images of these people are extracted from the VGG-Face dataset. Next, they are cropped to only contain the face, which is done in an automated fashion by using the Haar Cascade object detection algorithm (not further discussed). \n",
        "\n",
        "Throughout the rest of this tutorial, these people will be identified as persons A, B, C and D, respectively. They are specially selected so persons A & C and persons B & D share some characteristics, while persons A & B and persons C & D are very dissimilar. We chose these particular people because person A & C are both men while person B and D are both women. We will use 20 training images of both person A and person B. Each person also has 10 test images.\n",
        "\n",
        "The base images of all people are stored in the variable *images*, the faces are stored in the variable *images_face*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGBkRUir9f7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "from google.colab.patches import cv2_imshow\n",
        "from urllib import request\n",
        "\n",
        "base_path = \"/content/sample_data/CV__Group_assignment\"\n",
        "\n",
        "if not os.path.isdir(base_path):\n",
        "  os.makedirs(base_path)\n",
        "\n",
        "vgg_face_dataset_url = \"http://www.robots.ox.ac.uk/~vgg/data/vgg_face/vgg_face_dataset.tar.gz\"\n",
        "\n",
        "with request.urlopen(vgg_face_dataset_url) as r, open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\"), 'wb') as f:\n",
        "  f.write(r.read())\n",
        "\n",
        "with tarfile.open(os.path.join(base_path, \"vgg_face_dataset.tar.gz\")) as f:\n",
        "  f.extractall(os.path.join(base_path))\n",
        "\n",
        "trained_haarcascade_url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "\n",
        "with request.urlopen(trained_haarcascade_url) as r, open(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"), 'wb') as f:\n",
        "  f.write(r.read())\n",
        "\n",
        "all_subjects = [(\"Ang_Lee\", 32), (\"Natalie_Portman\", 38), (\"Jack_Ma\", 11), (\"Miranda_Kerr\", 11)]\n",
        "\n",
        "images = []\n",
        "for subject, nb_images in all_subjects:\n",
        "\n",
        "  with open(os.path.join(base_path, \"vgg_face_dataset\", \"files\", subject + \".txt\"), 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  images_ = []\n",
        "  for line in lines:\n",
        "    url = line[line.find(\"http://\"): line.find(\".jpg\") + 4]\n",
        "    print(url)\n",
        "\n",
        "    try:\n",
        "      res = request.urlopen(url, timeout=3)\n",
        "      img = np.asarray(bytearray(res.read()), dtype=\"uint8\")\n",
        "      img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "      h, w = img.shape[:2]\n",
        "      images_.append(img)\n",
        "      cv2_imshow(cv2.resize(img, (w // 5, h // 5)))\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    if len(images_) == nb_images:\n",
        "      images.append(images_)\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_GHYB7otduD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete doubles and unsuitable images from the database\n",
        "# Unsuitable images were identified visually\n",
        "unwanted_A = [3,12]\n",
        "unwanted_B = [5,8,8,10,15,17,18,22]\n",
        "unwanted_C = [9]\n",
        "\n",
        "for index in unwanted_A:\n",
        "  del images[0][index]\n",
        "\n",
        "for index in unwanted_B:\n",
        "  del images[1][index]\n",
        "\n",
        "for index in unwanted_C:\n",
        "  del images[2][index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G582IgL-fvi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faceCascade = cv2.CascadeClassifier(os.path.join(base_path, \"haarcascade_frontalface_default.xml\"))\n",
        "\n",
        "images_face = []\n",
        "\n",
        "for images_ in images:\n",
        "  faces_ = []\n",
        "  for img in images_:\n",
        "    img_ = img.copy()\n",
        "    img_gray = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)\n",
        "    faces = faceCascade.detectMultiScale(\n",
        "        img_gray,\n",
        "        scaleFactor=1.2,\n",
        "        minNeighbors=5,\n",
        "        minSize=(30, 30),\n",
        "        flags=cv2.CASCADE_SCALE_IMAGE\n",
        "    )\n",
        "    print(\"Found {} face(s)!\".format(len(faces)))\n",
        "\n",
        "    if len(faces) > 1:\n",
        "      faces = max(faces, key=lambda x: x[2]*x[3])\n",
        "      faces = [faces]\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "      cv2.rectangle(img_, (x, y), (x+w, y+h), (0, 255, 0), 10)\n",
        "      y_new = y - 10\n",
        "      h_new = h + 10\n",
        "      x_new = x - 10\n",
        "      w_new = w + 10\n",
        "      image_face = img[y_new:(y+h_new),x_new:(x+w_new)]\n",
        "      faces_.append(image_face)\n",
        "      cv2_imshow(cv2.resize(image_face, (w // 5, h // 5)))\n",
        "\n",
        "    h, w = img_.shape[:2]\n",
        "    cv2_imshow(cv2.resize(img_, (w // 5, h // 5)))\n",
        "\n",
        "  images_face.append(faces_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDieYg2jtJBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_database(images_i, title, person,columns,rows):\n",
        "  w = 10\n",
        "  h = 10\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "  fig.suptitle(title, fontsize=16)\n",
        "  for i in range(1, columns*rows +1):\n",
        "      img = images_i[person][i-1]\n",
        "      fig.add_subplot(rows, columns, i)\n",
        "      plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "plot_database(images_face, 'Person A training and test set', 0, 6, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZzihJS3tMbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_database(images_face, 'Person B training and test set', 1, 4, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zPKf9W1tQX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_database(images_face, 'Person C', 2, 2, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2RWEKnvtVVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_database(images_face, 'Person D', 3, 2, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qsVTRd0CQO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images[1] = images[1][:30]\n",
        "images[3] = images[3][:10]\n",
        "\n",
        "for images_ in images:\n",
        "  print(len(images_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SvVmvc3eR_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for faces_ in images_face:\n",
        "  print(len(faces_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw70pNjU09kM",
        "colab_type": "text"
      },
      "source": [
        "# Scale Invariant Feature Transform (SIFT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNHeAs7Y1CLW",
        "colab_type": "text"
      },
      "source": [
        "The Scale Invariant Feature Transform (SIFT) algorithm is an example of a handcrafted feature representation. It transforms an image into a set of local feature vectors and compares images in a series of steps:\n",
        "\n",
        "- Keypoint detection\n",
        "- Keypoint suppression\n",
        "- Keypoint description\n",
        "- Keypoint matching\n",
        "\n",
        "In the first phase, keypoints are detected at different scales of the image. This is done through a blob detection algorithm that is based on finding extrema of the Laplacian of the Gaussian-filtered image. These extrema are found at a certain characteristic scale $\\sigma$ of the Gaussian which denotes the size of the region at which the blob is most pronounced. This detection in scale-space is the reason why the features are scale-invariant, as the same keypoint can be detected and described at a different scale after a scaling transformation. Moreover, because the Laplacian operation is rotationally invariant, so will be the feature points.\n",
        "\n",
        "The first step results in many detected keypoints in position-scale space. In order to only retain the most prominent and robust keypoints for increased accuracy and efficiency, in the second step, we require that the contrast at the keypoint is high enough. For translation invariance, we also don't want keypoints to be on edges. However, the Laplacian response will often be high on edges. A classic approach is to look at the eigenvalues of the Hessian matrix at the position and the scale of a keypoint. It is well-known that in a corner-like structure, the eigenvalues will both be large (indicating the large change in both the x- and y-direction), whereas for an edge, only one eigenvalue is large. We only retain the keypoint when the ratio of the larger to the smaller eigenvalue is sufficiently small.\n",
        "\n",
        "For the remaining keypoints, a (SIFT) descriptor is constructed based on the gradient information in a neighbourhood around the keypoint (usually a $16 \\times 16$ patch). This patch is divided into 16 $4 \\times 4$ patches, where a histogram of gradients is constructed over 8 directions. This process results in a $4 * 4 * 8 = 128$-dimensional keypoint descriptor. The orientation of the gradients is determined with respect to a dominant gradient direction in the larger patch, which makes the descriptor rotation-invariant. We can threshold the numbers for illumination invariance. The histogram (binning) results in increased robustness to noise. The descriptor is, to a certain extent, also invariant to affine transformations.\n",
        "\n",
        "Finally, images can be compared by executing this process for both images and trying to match keypoints (descriptors) to detect whether an object (or in our case: a face) in the first image is also present in the second image. Matching can be based on a common distance measure such as the Euclidean distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5xaoQ3o420C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainImageA = images_face[0][:20]    # 20 training images of person A\n",
        "testImageA  = images_face[0][20:]    # 10 test images of person A\n",
        "trainImageB = images_face[1][:20]    # 20 training images of person B\n",
        "testImageB  = images_face[1][20:]    # 10 test images of person B\n",
        "testImageC  = images_face[2]         # 10 test images of person C who looks similar to Person A\n",
        "testImageD  = images_face[3]         # 10 test images of person D who looks similar to Person B"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc4gdeAILBRv",
        "colab_type": "text"
      },
      "source": [
        "We can use the built-in function SIFT_create to find the SIFT features. It accepts a couple of input parameters:\n",
        "\n",
        "- nfeatures: The number of feature points to retain.\n",
        "- nOctaveLayers: The number of layers in each octave. An octave corresponds to a certain scale. This is used for building a computationally efficient representation of the Laplacian of the Gaussian by a Difference of Gaussians.\n",
        "- contrastThreshold: The contrast threshold used to filter out weak features in low-contrast regions. The larger the threshold, the less features are produced by the detector.\n",
        "- edgeThreshold: The threshold used to filter out edge-like features. The larger the threshold, the more features are retained.\n",
        "- sigma: The width of the Gaussian applied to the input image at the first octave."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRF66P3n4ob0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_sift(img, features=20, octave=10, contrast=0.03, edge=10, sigma=1):\n",
        "    sift = cv2.xfeatures2d.SIFT_create(\n",
        "        nfeatures=features, nOctaveLayers=octave, contrastThreshold=contrast, edgeThreshold=edge, sigma=sigma)\n",
        "    kp, des = sift.detectAndCompute(img, None)\n",
        "    imgFeatured = cv2.drawKeypoints(img, kp, None)\n",
        "    plt.imshow(imgFeatured)\n",
        "    plt.show()\n",
        "\n",
        "    return kp, des"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoR9OkS0M92u",
        "colab_type": "text"
      },
      "source": [
        "In order to train a binary classifier that can recognise person A (class 0) and person B (class 1), we need to construct the SIFT features for the training and the test data. The detected feature points are overlaid on the face images. We see that they indeed correspond to regions that are visually salient. In particular, the eyes, nose and mouth are often described by several feature points. However, also points on clothes, the environment and any occluding objects are sometimes incorporated, which we can expect to decrease performance. Importantly, we can assume that the feature descriptors satisfy the invariance properties discussed above, which will be important when classifying and comparing the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SUq0ugt4tT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getSiftFeatureAsXY(imageList):\n",
        "    Xtrain = []\n",
        "    for img in imageList:\n",
        "      # compute SIFT keypoints & descriptors for image\n",
        "      kp, des = extract_sift(img)\n",
        "      Xtrain.append((des.astype(float)[:20]).ravel()) # make sure there are only 20 feature points\n",
        "    return Xtrain\n",
        "\n",
        "XtrainA = getSiftFeatureAsXY(trainImageA)\n",
        "XtrainB = getSiftFeatureAsXY(trainImageB)\n",
        "XtestA  = getSiftFeatureAsXY(testImageA)\n",
        "XtestB  = getSiftFeatureAsXY(testImageB)\n",
        "XtestC  = getSiftFeatureAsXY(testImageC)\n",
        "XtestD  = getSiftFeatureAsXY(testImageD)\n",
        "YtrainA = [0] * len(trainImageA)\n",
        "YtrainB = [1] * len(trainImageB)\n",
        "YtestA  = [0] * len(testImageA)\n",
        "YtestB  = [1] * len(testImageB)\n",
        "YtestC  = [0] * len(testImageC)\n",
        "YtestD  = [1] * len(testImageD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4iICLJQ6LF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtrain  = XtrainA + XtrainB\n",
        "Ytrain  = YtrainA + YtrainB\n",
        "XtestAB = XtestA  + XtestB\n",
        "YtestAB = YtestA  + YtestB\n",
        "XtestCD = XtestC  + XtestD\n",
        "YtestCD = YtestC  + YtestD\n",
        "\n",
        "Xtrain  = np.array(Xtrain)\n",
        "Ytrain  = np.array(Ytrain)\n",
        "XtestAB = np.array(XtestAB)\n",
        "YtestAB = np.array(YtestAB)\n",
        "XtestCD = np.array(XtestCD)\n",
        "YtestCD = np.array(YtestCD)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-P2fS9bqTbD",
        "colab_type": "text"
      },
      "source": [
        "For tuning the descriptor, we apply feature matching for two images of the same person. If there are more matched features, then these descriptors are the most important ones that need to be kept. We adjust the parameters of the SIFT_create function until feature matching performs well enough for this person. The image below shows good matching correspondence. However, it is constructed for two faces that are very similar in pose and background, so we don't expect performance to be as well when testing on the whole dataset.\n",
        "\n",
        "We found that applying blurring as a pre-processing step on the images will make some feature matching more accurate. However, in general it will reduce the number of features that are detected using the SIFT algorithm. We would like to keep the high-contrast points as salient as possible. Also, because binning over a larger patch increases robustness to noise, we chose not to apply any pre-processing for SIFT.\n",
        "\n",
        "For detecting an object of interest in a new image, we perform feature matching with the SIFT features. Then we calculate the distance of the descriptors for the matches to assess the quality of the matches. More good matches imply that the features are more representative.\n",
        "\n",
        "![](https://i.imgur.com/XbklZhh.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KNxyxY2qVok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def siftFeatureMatching(img1, img2):\n",
        "    \n",
        "    kp1, des1 = extract_sift(img1, 500)\n",
        "    kp2, des2 = extract_sift(img2, 500)\n",
        "\n",
        "    # Create BFMatcher object\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
        "\n",
        "    # Match descriptors\n",
        "    matches = bf.knnMatch(des1, des2, k=2)\n",
        "\n",
        "    # Apply ratio test\n",
        "    good = []\n",
        "    for m, n in matches:\n",
        "        if m.distance < 0.75 * n.distance:\n",
        "            good.append([m])\n",
        "            \n",
        "    # draw image with features matching\n",
        "    imgMatching = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good, None, flags=2)\n",
        "    cv2_imshow(imgMatching)\n",
        "\n",
        "siftFeatureMatching(trainImageA[10], trainImageA[14])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fODyqwDG6S2v",
        "colab_type": "text"
      },
      "source": [
        "## Classification & Identification\n",
        "\n",
        "We use the sklearn python package to build the binary classifiers and calculate the accuracy score for each classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdIpVzsAQwjS",
        "colab_type": "text"
      },
      "source": [
        "Since the SIFT algorithm has a strong invariance to scaling, rotation, brightness changes and noise, we hope to achieve good generalization to the test images of persons A & B and even to the more dissimilar images of persons C and D. The classification accuracy is 85% for persons A and B, but it is only 70% for persons C and D, using a support vector machine. We choose a support vector machine because they are generally well-suited for high-dimensional data, which is the case for 128-dimensional SIFT features and 20 feature points per image.\n",
        "\n",
        "Although C may look similar to A and D may look similar to B, they are still different persons. It is normal that the SIFT keypoints for person A are quite different than the ones for person C (idem for persons B and D). Since we train the classifier only with images of persons A and B, the classifier does not perform well for C and D. This shows us that the generalization performance of a handcrafted feature representation like SIFT is not very good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL2AXna1Xrfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm\n",
        "\n",
        "# training phase\n",
        "clf = svm.SVC(kernel=\"linear\")\n",
        "clf.fit(Xtrain, Ytrain)\n",
        "\n",
        "# predict labels\n",
        "print(\"Accuracy for SVM\")\n",
        "predictAB = clf.predict(XtestAB)\n",
        "predictCD = clf.predict(XtestCD)\n",
        "\n",
        "# compute accuracy\n",
        "print('Correct:  ', YtestAB)\n",
        "print('Predicted:', predictAB)\n",
        "accuracy = accuracy_score(YtestAB, predictAB) * 100\n",
        "print(\"Accuracy for persons A & B: %.2f\" % accuracy + \"%\\n\")\n",
        "\n",
        "print('Correct:  ', YtestCD)\n",
        "print('Predicted:', predictCD)\n",
        "accuracy = accuracy_score(YtestCD, predictCD) * 100\n",
        "print(\"Accuracy for persons C & D: %.2f\" % accuracy + \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvgaCcumpMim",
        "colab_type": "text"
      },
      "source": [
        "Next, we try to perform classification with an MLP with 3 hidden layers including of 10, 8 and 3 neurons respectively. The performance gets slightly better as compared to the SVM classifier. The accuracy for persons A and B is 85%, while that for persons C and D is 75%. For two of the images for person A that is classified as person B, one of them is half-blocked by a trophy while another is half-blocked by a shadow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3asBAgOPpNUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_sift = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(10,8,3), random_state=1)\n",
        "mlp_sift.fit(Xtrain, Ytrain)\n",
        "\n",
        "# predict labels\n",
        "print(\"Accuracy for MLP (hidden layer: 10+8+3)\")\n",
        "predictAB = mlp_sift.predict(XtestAB)\n",
        "predictCD = mlp_sift.predict(XtestCD)\n",
        "\n",
        "# compute accuracy\n",
        "print(np.array(predictAB))\n",
        "accuracy = accuracy_score(YtestAB, predictAB) * 100\n",
        "print(\"Accuracy for persons A & B: %.2f\" % accuracy + \"%\\n\")\n",
        "\n",
        "print(np.array(predictCD))\n",
        "accuracy = accuracy_score(YtestCD, predictCD) * 100\n",
        "print(\"Accuracy for persons C & D: %.2f\" % accuracy + \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIuTFkRNqy0p",
        "colab_type": "text"
      },
      "source": [
        "Finally, we perform KNN with $K = 5$. The accuracies are exactly the same as for the SVM. To learn more about these results, we use TSNE to visualize the feature distribution. We label the test images of persons A and B with labels 2 and 3. We combine them with the training images, labelled 0 and 1. We can see in the plot that the test images of person A (green) are close to the training images of person A (blue), and the same holds for person B (red and orange). This explains the high accuracy.\n",
        "\n",
        "For persons C and D (second plot), there is more interference: the region for persons A & C and the region for persons B & D is less clearly delineated. This explains the lower accuracy in that case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJeldP2_6khX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# training phase\n",
        "neigh = KNeighborsClassifier(n_neighbors=5)\n",
        "neigh.fit(Xtrain, Ytrain)\n",
        "\n",
        "# predict labels\n",
        "print(\"Accuracy for KNN (K = 5)\")\n",
        "predictAB = neigh.predict(XtestAB)\n",
        "predictCD = neigh.predict(XtestCD)\n",
        "\n",
        "# compute accuracy\n",
        "print('Correct:  ', YtestAB)\n",
        "print('Predicted:', predictAB)\n",
        "accuracy = accuracy_score(YtestAB, predictAB) * 100\n",
        "print(\"Accuracy for persons A & B: %.2f\" % accuracy + \"%\\n\")\n",
        "\n",
        "print('Correct:  ', YtestCD)\n",
        "print('Predicted:', predictCD)\n",
        "accuracy = accuracy_score(YtestCD, predictCD) * 100\n",
        "print(\"Accuracy for persons C & D: %.2f\" % accuracy + \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-TvSwmAs4X5",
        "colab_type": "text"
      },
      "source": [
        "## SIFT features visualization\n",
        "\n",
        "We present t-SNE for visualization. The details of t-SNE can be found in the section *Transfer Learning: t-SNE visualization*. \n",
        "\n",
        "Here we can see that it's easier to discriminate training data from test data of persons A and B visually. It also explains the relatively low classification accuracy of our classifiers for persons C and D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soBcm-HN69kS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "\n",
        "# ---------------------------------\n",
        "#   For testing of person A and B\n",
        "# ---------------------------------\n",
        "\n",
        "\n",
        "feature = np.concatenate((Xtrain, XtestAB))\n",
        "feature = feature.reshape((60, 2560))\n",
        "\n",
        "# Defining Model\n",
        "tsne = TSNE(learning_rate=50, perplexity=5)\n",
        "\n",
        "# Fitting Model\n",
        "transformed = tsne.fit_transform(feature)\n",
        "\n",
        "# Plotting 2d t-Sne\n",
        "x_axis = transformed[:, 0]\n",
        "y_axis = transformed[:, 1]\n",
        "\n",
        "Ylabel = [2] * len(testImageA) + [3] * len(testImageB)\n",
        "Ylabel = np.array(Ylabel)\n",
        "label_tsne = np.concatenate((Ytrain, Ylabel))\n",
        "\n",
        "palette = sns.color_palette(\"bright\", 4)\n",
        "sns.scatterplot(x_axis, y_axis, hue=label_tsne, legend='full', palette=palette)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xr7tog-YrARU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------------\n",
        "#   For testing of person C and D\n",
        "# ---------------------------------\n",
        "\n",
        "feature = np.concatenate((Xtrain, XtestCD))\n",
        "feature = feature.reshape((60, 2560))\n",
        "\n",
        "# Defining Model\n",
        "tsne = TSNE(learning_rate=30, perplexity=5)\n",
        "\n",
        "# Fitting Model\n",
        "transformed = tsne.fit_transform(feature)\n",
        "\n",
        "# Plotting 2d t-Sne\n",
        "x_axis = transformed[:, 0]\n",
        "y_axis = transformed[:, 1]\n",
        "\n",
        "Ylabel = [2] * len(testImageC) + [3] * len(testImageD)\n",
        "Ylabel = np.array(Ylabel)\n",
        "label_tsne = np.concatenate((Ytrain, Ylabel))\n",
        "\n",
        "palette = sns.color_palette(\"bright\", 4)\n",
        "sns.scatterplot(x_axis, y_axis, hue=label_tsne, legend='full', palette=palette)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRt45iTey4p4",
        "colab_type": "text"
      },
      "source": [
        "Next, we again perform KNN ($K = 5$) with the mean-squared error between the principal component weights as a distance metric. We show the nearest neighbours of the test images of persons A and B. Here, only two of person A's images are not well clustered in feature space, where they are confused with person B. One of the images of his face is half-blocked by the trophy, while another image of his face is blocked by shadow.\n",
        "\n",
        "For the test images of persons C and D, only two of person C's images are identified as person B. However, most of person D's images are confused with person A."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCJ5qq-My44Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualizeKNN(k = 5):\n",
        "  print('Performance on the test images of persons A and B\\n')\n",
        "\n",
        "  train_data = []\n",
        "  for i in range(len(Xtrain)):\n",
        "    if i < 20:\n",
        "      train_data.append((Xtrain[i], 0, trainImageA[i]))\n",
        "    else:\n",
        "      train_data.append((Xtrain[i], 1, trainImageB[i-20]))\n",
        "  \n",
        "  test_data1 = []\n",
        "  testImageAB = testImageA + testImageB\n",
        "  for i in range(len(XtestAB)):\n",
        "    test_data1.append((XtestAB[i]))\n",
        "  \n",
        "  test_data2 = []\n",
        "  testImageCD = testImageC + testImageD\n",
        "  for i in range(len(XtestCD)):\n",
        "    test_data2.append((XtestCD[i]))\n",
        "\n",
        "  labels = []\n",
        "  for i in range(len(test_data1)):\n",
        "    sort = sorted(train_data, key=lambda x: np.mean(np.square(np.array(x[0])-np.array(test_data1[i]))))[:k]\n",
        "    sort_labels = [x[1] for x in sort]\n",
        "    sort_images = [x[2] for x in sort]\n",
        "    label = max(sort_labels, key=sort_labels.count)\n",
        "    labels.append(label)\n",
        "    print('ORIGINAL')\n",
        "    cv2_imshow(cv2.resize(testImageAB[i], (128, 128)))\n",
        "    print('NEIGHBOURS')\n",
        "    sort_images = [cv2.resize(img, (128, 128)) for img in sort_images]\n",
        "    neighbours = np.hstack((sort_images[0], sort_images[1], sort_images[2], sort_images[3], sort_images[4]))\n",
        "    cv2_imshow(neighbours)\n",
        "\n",
        "  print('\\nCorrect:  ', YtestAB.tolist())\n",
        "  print('Predicted:', labels)\n",
        "  frac_match = np.mean([1 if YtestAB[i] == labels[i] else 0 for i in range(len(labels))])\n",
        "  print('Accuracy: ', frac_match)\n",
        "\n",
        "  print('\\nPerformance on the images of persons C and D\\n')\n",
        "\n",
        "  labels = []\n",
        "  for i in range(len(test_data2)):\n",
        "    sort = sorted(train_data, key=lambda x: np.mean(np.square(np.array(x[0])-np.array(test_data2[i]))))[:k]\n",
        "    sort_labels = [x[1] for x in sort]\n",
        "    sort_images = [x[2] for x in sort]\n",
        "    label = max(sort_labels, key=sort_labels.count)\n",
        "    labels.append(label)\n",
        "    print('ORIGINAL')\n",
        "    cv2_imshow(cv2.resize(testImageCD[i], (128, 128)))\n",
        "    print('NEIGHBOURS')\n",
        "    sort_images = [cv2.resize(img, (128, 128)) for img in sort_images]\n",
        "    neighbours = np.hstack((sort_images[0], sort_images[1], sort_images[2], sort_images[3], sort_images[4]))\n",
        "    cv2_imshow(neighbours)\n",
        "\n",
        "  print('\\nCorrect:  ', YtestCD.tolist())\n",
        "  print('Predicted:', labels)\n",
        "  frac_match = np.mean([1 if YtestCD[i] == labels[i] else 0 for i in range(len(labels))])\n",
        "  print('Accuracy: ', frac_match)\n",
        "\n",
        "visualizeKNN(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypSp9QBuJmXd",
        "colab_type": "text"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq3V5jK3JxrG",
        "colab_type": "text"
      },
      "source": [
        "Principal Component Analysis (PCA) is a dimensionality reduction technique for high-dimensional input data. Mathematically, for linear PCA, we would like to find a matrix $D \\in R^{m \\times l}$ with $l < m$ such that $x \\approx DD^Tx$. $D^Tx$ is the projection of $x \\in R^m$ in the subspace $R^l$ such that the original point can be reconstructed in the best possible way.\n",
        "\n",
        "Finding this matrix $D$ comes down to solving the eigenvalue problem of $XX^T$ with $X \\in R^{m \\times n}$ the matrix containing $n$ training data points $x_i \\in R^m$. $XX^T$ has $m$ eigenvalues $\\lambda_i$ and corresponding eigenvectors $u_i \\in R^m$, and $D$ is constructed by retaining only the eigenvectors corresponding to the $l$ largest eigenvalues. These eigenvectors are the principal components of the training data.\n",
        "\n",
        "Visually, the principal components are the directions in the data with the highest variance. They span an $l$-dimensional subspace onto which the $m$-dimensional datapoints can be projected with minimal loss in information. In other words, the principal components carry the highest signal of the data, while the left-out eigenvectors carry few information or are due to noise in the data. The number of principal components $l$ forms a trade-off between the amount of dimensionality reduction (efficiency of the representation) and the amount of lost information (quality of the representation). We will explore this trade-off further.\n",
        "\n",
        "The image below shows all principal components of a two-dimensional dataset. Projecting the points onto only the most important direction clearly retains the maximal amount of information possible for a one-dimensional representation of the data.\n",
        "\n",
        "![[Source:](https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/GaussianScatterPCA.png/512px-GaussianScatterPCA.png)](https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/GaussianScatterPCA.png/512px-GaussianScatterPCA.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0zUGsw1aBqr",
        "colab_type": "text"
      },
      "source": [
        "As a pre-processing step for PCA to work, the faces need to be aligned, which means that the images of the faces are the same size and the eyes, nose, mouth, ... are more or less in the same place. The code to automate the alignment of the faces in our dataset is shown in the two cells below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgW02sAK8Sv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bz2\n",
        "\n",
        "shape_predictor_dataset = \"https://raw.github.com/davisking/dlib-models/master/shape_predictor_68_face_landmarks.dat.bz2\"\n",
        "\n",
        "with request.urlopen(shape_predictor_dataset) as r, open(os.path.join(base_path, \"shape_predictor_dataset.dat.bz2\"), 'wb') as f:\n",
        "  f.write(r.read())\n",
        "\n",
        "filepath = os.path.join(base_path, \"shape_predictor_dataset.dat.bz2\")\n",
        "zipfile = bz2.BZ2File(filepath) # open the file\n",
        "data = zipfile.read() # get the decompressed data\n",
        "newfilepath = filepath[:-4] # assuming the filepath ends with .bz2\n",
        "print(newfilepath)\n",
        "open(newfilepath, 'wb').write(data) # write to an uncompressed file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8sASfzl_I-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imutils\n",
        "from imutils.face_utils import FaceAligner\n",
        "import dlib\n",
        "\n",
        "# By Adrian Rosebrock\n",
        "# https://www.pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/\n",
        "def align(images):\n",
        "    # initialize dlib's face detector (HOG-based) and then create\n",
        "    # the facial landmark predictor and the face aligner\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(os.path.join(base_path, \"shape_predictor_dataset.dat\"))\n",
        "    fa = FaceAligner(predictor, desiredFaceWidth=256)\n",
        "    aligned = []\n",
        "    for image in images:\n",
        "        # resize & convert to grayscale\n",
        "        image = imutils.resize(image, width=800)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        # detect faces in the grayscale image\n",
        "        rects = detector(gray, 1)\n",
        "        \n",
        "        if len(rects) > 1:\n",
        "          rects = [max(rects, key=lambda x: x.area())]\n",
        "\n",
        "        # loop over the face detections\n",
        "        for rect in rects:\n",
        "            # extract the ROI of the *original* face, then align the face\n",
        "            # using facial landmarks\n",
        "            faceAligned = fa.align(image, gray, rect)\n",
        "            cv2_imshow(faceAligned)\n",
        "            aligned.append(faceAligned)\n",
        "    return aligned\n",
        "\n",
        "aligned_faces = []\n",
        "for faces_ in images_face:\n",
        "  aligned_faces.append(align(faces_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88n5c9yjbQNt",
        "colab_type": "text"
      },
      "source": [
        "The general method of PCA has also been applied for analyzing images of faces. Images can be transformed into a suitable vector representation by considering each colour channel of each pixel as a separate component. For example, a $100 \\times 100$ image results in a vector of $100 * 100 * 3 = 30000$ components. *image_to_vec* and *vec_to_image* achieve this transformation.\n",
        "\n",
        "Luckily, OpenCV has a function that conveniently calculates all information regarding PCA for us. Under the hood, the input images (vectors) are first transformed to have zero mean. Second, the previously discussed eigenvalue problem is solved. In this context, the returned principal components are called eigenfaces. The most important eigenfaces capture the most salient characteristics of the training faces.\n",
        "\n",
        "Finally, we can project a new face onto the calculated principal component subspace and reconstruct the face from this lower-dimensional representation. Calculating the dot product between the face vector and the principal components gives us the projection of the face in the lower-dimensional subspace. The PCA method makes sure that this is done in an optimal way in the sense that we lose as little information as possible when performing this dimensionality reduction.\n",
        "\n",
        "Note that the images need to be normalized to have zero mean. This is because the mean is a consequence of the translation of the data points, which cannot be accounted for when performing variance analysis and placing the principal component basis' origin. A test image also needs to be displaced according to the mean before processing, and be displaced backwards afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv4LpuGeEmSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def image_to_vec(image):\n",
        "    row = []\n",
        "    for i in range(image.shape[0]):  # x\n",
        "        for j in range(image.shape[1]):  # y\n",
        "            for k in range(image.shape[2]):  # Colour\n",
        "                row.append(image[i][j][k])\n",
        "    return row\n",
        "\n",
        "\n",
        "def vec_to_image(image_as_row):\n",
        "    size = np.sqrt(image_as_row.size / 3).astype(int)\n",
        "    image = np.zeros((size, size, 3), np.float32)\n",
        "    for i in range(size):\n",
        "        for j in range(size):\n",
        "            for k in range(3):\n",
        "                image[i][j][k] = image_as_row[i * size * 3 + j * 3 + k]\n",
        "    return image\n",
        "\n",
        "\n",
        "def pca(train_images, nr_components):\n",
        "    print(\"Starting PCA with\", str(nr_components), \"components\")\n",
        "    train_matrix = [image_to_vec(\n",
        "        cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, cv2.CV_8UC3)\n",
        "    ) for img in train_images]\n",
        "    train_matrix = np.array(train_matrix)\n",
        "    # Mean is automatically calculated from the data\n",
        "    mean, eigen_faces = cv2.PCACompute(train_matrix, mean=None, maxComponents=nr_components)\n",
        "    # print(\"\\nEigenvalues:\\n\", vals)\n",
        "    return mean[0], eigen_faces\n",
        "\n",
        "\n",
        "def reconstruct(img, mean, eigen_faces):\n",
        "    img = cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, cv2.CV_8UC3)\n",
        "    img_as_row = np.array(image_to_vec(img))\n",
        "    rec = mean.copy()\n",
        "    weights = []\n",
        "    for vec in eigen_faces:\n",
        "        weight = np.dot((img_as_row - mean), vec)\n",
        "        weights.append(weight)\n",
        "        rec += weight * vec\n",
        "    rec = vec_to_image(rec)\n",
        "    error = np.mean((np.square(img - rec)))\n",
        "    rec = cv2.normalize(rec, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC3)\n",
        "    print(\"\\nRECONSTRUCTED\")\n",
        "    cv2_imshow(rec)\n",
        "    print('\\nError:', error)\n",
        "    print('------------------------\\n\\n')\n",
        "    return error, weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsUWDoRRi_Vx",
        "colab_type": "text"
      },
      "source": [
        "Here, we reconstruct one of the test images of person A based on an increasing number of considered eigenfaces of the training images. We see that increasing the number of dimensions indeed reduces the error, and most significantly so with the first few eigenfaces. In this case, apparently, the third eigenface of person A captures some important characteristics of the test image, as the mean squared error drops the most and the image changes visually when we include that dimension. With a very aggressive dimensionality reduction, this may be the optimal number of components to take into account. Although the reconstruction is obviously not complete with the first 15 principal components, consider already the quality of the result with only 15 of $256 * 256 * 3 = 196608$ dimensions. Also note that we have a very challenging problem, as the training and test faces don't appear in a very uniform way that is optimal for PCA (faces turned, rotated, object partially in front of the face, high variance in the background, ...)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3m-wpoOGyQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def get_eigen_faces():\n",
        "  train = aligned_faces[0][:20] + aligned_faces[1][:20]\n",
        "  test = aligned_faces[0][20:] + aligned_faces[1][20:]\n",
        "  test_image = test[1]\n",
        "  print(\"ORIGINAL\")\n",
        "  cv2_imshow(test_image)\n",
        "\n",
        "  max_components = 15\n",
        "  eigen_faces = []\n",
        "  errors = []\n",
        "  for i in range(max_components):\n",
        "      mean, vecs = pca(train, i+1)\n",
        "      error, _ = reconstruct(test_image, mean, vecs)\n",
        "      errors.append(error)\n",
        "      eigen_faces = vecs\n",
        "\n",
        "  plt.plot(np.linspace(1, max_components, max_components), errors)\n",
        "  plt.show()\n",
        "  return train, test, mean, eigen_faces\n",
        "\n",
        "train, test, mean, eigen_faces = get_eigen_faces()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYKZ2V6Vp0GU",
        "colab_type": "text"
      },
      "source": [
        "## Eigenfaces Visualization\n",
        "\n",
        "Let's visualize the eigenfaces and observe how the first eigenfaces capture some very salient characteristics of the set of training faces, while less important eigenfaces seem to capture more detailed information about individual images and characteristics that could be attributed to noise and variance because of the heterogeneous setting in which the images were taken. Some eigenfaces clearly capture man-like characteristics (person A), other eigenfaces clearly capture woman-like characteristics (person B)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTkFC7WgkAKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for ef in eigen_faces:\n",
        "  cv2_imshow(cv2.normalize(vec_to_image(ef), None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8UC3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdClQiLNLIwP",
        "colab_type": "text"
      },
      "source": [
        "We now visualize the first two principal components of the test images of persons A and B. While person B is quite well clustered in the left half of the figure, there is more variance for person A. This is not necessarily a large problem when performing classification with more principal components (as we saw in the error graph above, the next principal components can still significantly improve the representation), but nevertheless it can provide an explanation when we notice that certain errors are made. As we will see, this is mostly the case in our nearest neighbours classifier in the next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HyEWTSUogSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = []\n",
        "for img in test:\n",
        "  _, w = reconstruct(img, mean, eigen_faces)\n",
        "  weights.append(w[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9fmP6QGkCl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bokeh.plotting import figure\n",
        "from bokeh.io import output_notebook, show\n",
        "\n",
        "# Call once to configure Bokeh to display plots inline in the notebook.\n",
        "output_notebook()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roOqhkOAmM2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_image(image, angle):\n",
        "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "  return result\n",
        "\n",
        "test_rgba = [np.array(cv2.cvtColor(rotate_image(img, 180), cv2.COLOR_BGR2RGBA)) for img in test]\n",
        "weightsX = [w[0] for w in weights]\n",
        "weightsY = [w[1] for w in weights]\n",
        "\n",
        "p = figure(plot_width=600, plot_height=600, x_range=(-80,80), y_range=(-80,80))\n",
        "\n",
        "p.image_rgba(test_rgba, weightsX, weightsY, [10]*len(test_rgba), [10]*len(test_rgba))\n",
        "\n",
        "# show the results\n",
        "show(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gw_8I6jOE3e",
        "colab_type": "text"
      },
      "source": [
        "## Classification & Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAPrmUdS3K09",
        "colab_type": "text"
      },
      "source": [
        "Let's again try to perform classification and identification using the PCA features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK8sI7OYN5Cy",
        "colab_type": "text"
      },
      "source": [
        "Redefine the important functions without visualizations, initialize the data and train the PCA identifier on the training images of persons A and B. *test_data1* contains the feature representations of persons A and B, *test_data2* contains the feature representations of persons C and D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as4cmVw_3Slr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pca(train_images, nr_components):\n",
        "    print(\"Starting PCA with\", str(nr_components), \"components\")\n",
        "    train_matrix = [image_to_vec(\n",
        "        cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, cv2.CV_8UC3)\n",
        "    ) for img in train_images]\n",
        "    train_matrix = np.array(train_matrix)\n",
        "    # Mean is automatically calculated from the data\n",
        "    mean, eigen_faces = cv2.PCACompute(train_matrix, mean=None, maxComponents=nr_components)\n",
        "    return mean[0], eigen_faces\n",
        "\n",
        "\n",
        "def reconstruct(img, mean, eigen_faces):\n",
        "    img = cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, cv2.CV_8UC3)\n",
        "    img_as_row = np.array(image_to_vec(img))\n",
        "    weights = []\n",
        "    for vec in eigen_faces:\n",
        "        weight = np.dot((img_as_row - mean), vec)\n",
        "        weights.append(weight)\n",
        "    return weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4o9oNXp6Sb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = aligned_faces[0][:20].copy()\n",
        "train.extend(aligned_faces[1][:20].copy())\n",
        "\n",
        "test1 = aligned_faces[0][20:].copy()\n",
        "test1.extend(aligned_faces[1][20:].copy())\n",
        "correct1 = ([0] * len(aligned_faces[0][20:]))\n",
        "correct1.extend([1] * len(aligned_faces[1][20:]))\n",
        "\n",
        "test2 = aligned_faces[2].copy()\n",
        "test2.extend(aligned_faces[3].copy())\n",
        "correct2 = ([0] * len(aligned_faces[2]))\n",
        "correct2.extend([1] * len(aligned_faces[3]))\n",
        "\n",
        "mean, eigen_faces = pca(train, 15)\n",
        "train_data = []\n",
        "train_data_w = []\n",
        "for i in range(len(train)):\n",
        "  weights = reconstruct(train[i], mean, eigen_faces)\n",
        "  if i < 20:\n",
        "    train_data.append((weights, 0, train[i]))\n",
        "    train_data_w.append(weights)\n",
        "  else:\n",
        "    train_data.append((weights, 1, train[i]))\n",
        "    train_data_w.append(weights)\n",
        "\n",
        "test_data1 = []\n",
        "for img in test1:\n",
        "  weights = reconstruct(img, mean, eigen_faces)\n",
        "  test_data1.append(weights)\n",
        "\n",
        "test_data2 = []\n",
        "for img in test2:\n",
        "  weights = reconstruct(img, mean, eigen_faces)\n",
        "  test_data2.append(weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Ynfhu6u5S3",
        "colab_type": "text"
      },
      "source": [
        "Like before, we apply an SVM classifier first. The input features in this case are the principal component weights. It can be seen that 85% of person A and B are correctly classified, while the accuracy for person C and D is 65%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSG7CsJEu6ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = [x[0] for x in train_data]\n",
        "y = [x[1] for x in train_data]\n",
        "\n",
        "clf = svm.SVC(kernel=\"linear\")\n",
        "clf.fit(X, y)\n",
        "\n",
        "print('Performance on the test images of persons A and B')\n",
        "\n",
        "labels = clf.predict(test_data1)\n",
        "print('Correct:  ', correct1)\n",
        "print('Predicted:', labels.tolist())\n",
        "frac_match = np.mean([1 if correct1[i] == labels[i] else 0 for i in range(len(labels))])\n",
        "print('Accuracy: ', frac_match)\n",
        "\n",
        "print('\\nPerformance on the images of persons C and D')\n",
        "\n",
        "labels = clf.predict(test_data2)\n",
        "print('Correct:  ', correct2)\n",
        "print('Predicted:', labels.tolist())\n",
        "frac_match = np.mean([1 if correct2[i] == labels[i] else 0 for i in range(len(labels))])\n",
        "print('Accuracy: ', frac_match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMmyNXlFoaQF",
        "colab_type": "text"
      },
      "source": [
        "Next, let's try to classify the test images with a 2-layer neural network.  We see that this network performs very well on the test images of persons A and B. For the images of persons C and D, which are selected to be similar to persons A and B respectively, the classifier achieves an accuracy of 75%. These results are better than those achieved with SIFT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMBwovqt9z-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "X = [x[0] for x in train_data]\n",
        "y = [x[1] for x in train_data]\n",
        "\n",
        "clf = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(8,6), random_state=1)\n",
        "clf.fit(X, y)\n",
        "\n",
        "print('Performance on the test images of persons A and B')\n",
        "\n",
        "labels = clf.predict(test_data1)\n",
        "print('Correct:  ', correct1)\n",
        "print('Predicted:', labels.tolist())\n",
        "frac_match = np.mean([1 if correct1[i] == labels[i] else 0 for i in range(len(labels))])\n",
        "print('Accuracy: ', frac_match)\n",
        "\n",
        "print('\\nPerformance on the images of persons C and D')\n",
        "\n",
        "labels = clf.predict(test_data2)\n",
        "print('Correct:  ', correct2)\n",
        "print('Predicted:', labels.tolist())\n",
        "frac_match = np.mean([1 if correct2[i] == labels[i] else 0 for i in range(len(labels))])\n",
        "print('Accuracy: ', frac_match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdm9FowpOmGI",
        "colab_type": "text"
      },
      "source": [
        "Next, we perform 3-NN with the mean-squared error between the principal component weights as a distance metric. We show the nearest neighbours of the test images of persons A and B. Apparently, person A's images are not well clustered in feature space, as he is often confused with person B. When we refer to the visualization of the first two principal components, this is indeed what we could have expected.\n",
        "\n",
        "The pretty bad performance of PCA for nearest neighbours identification can furthermore be explained by the fact that we use an unweighted Euclidean distance when calculating the distance between the principal component weights. For PCA however, it would be useful to give more importance to the first principal components as they should be more discriminative for classification. This could be a useful extension to investigate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpmDQmFtKXc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 3\n",
        "\n",
        "print('Performance on the test images of persons A and B')\n",
        "\n",
        "labels = []\n",
        "for i in range(len(test_data1)):\n",
        "  sort = sorted(train_data, key=lambda x: np.mean(np.square(np.array(x[0])-np.array(test_data1[i]))))[:k]\n",
        "  sort_labels = [x[1] for x in sort]\n",
        "  sort_images = [x[2] for x in sort]\n",
        "  label = max(sort_labels, key=sort_labels.count)\n",
        "  labels.append(label)\n",
        "  print('ORIGINAL')\n",
        "  cv2_imshow(test1[i])\n",
        "  print('NEIGHBOURS')\n",
        "  neighbours = np.hstack((sort_images[0], sort_images[1], sort_images[2]))\n",
        "  cv2_imshow(neighbours)\n",
        "\n",
        "print('Correct:  ', correct1)\n",
        "print('Predicted:', labels)\n",
        "frac_match = np.mean([1 if correct1[i] == labels[i] else 0 for i in range(len(labels))])\n",
        "print('Accuracy: ', frac_match)\n",
        "\n",
        "print('\\nPerformance on the images of persons C and D')\n",
        "\n",
        "labels = []\n",
        "for img in test_data2:\n",
        "  sort = sorted(train_data, key=lambda x: np.mean(np.square(np.array(x[0])-np.array(img))))[:k]\n",
        "  sort = [x[1] for x in sort]\n",
        "  label = max(sort, key=sort.count)\n",
        "  labels.append(label)\n",
        "\n",
        "print('\\nCorrect:  ', correct2)\n",
        "print('Predicted:', labels)\n",
        "frac_match = np.mean([1 if correct2[i] == labels[i] else 0 for i in range(len(labels))])\n",
        "print('Accuracy: ', frac_match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvqVqCh5JRbR",
        "colab_type": "text"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "To define transfer learning, we first introduce some notation. A domain $D$ consists of two components: a feature space $\\mathcal{X}$ and a marginal probability distribution $P(X)$, where $X = \\{x_{1}, ... x_{n}\\} \\in \\mathcal{X}$. The objective function is defined as $f()$.\n",
        "\n",
        "Given a source domain $D_S$ and learning task $T_S$, a target domain $D_T$ and learning task $T_T$ , transfer learning aims to help improve the learning of the target predictive function $f_{T}()$ in $D_T$ using the knowledge in $D_S$ and $T_S$, where $D_{S} \\neq D_{T}$, or $T_{S} \\neq T_{T}$. (Pan & Yang, 2010) This means that we transfer the knowledge learned in the context of one task to improve performance on a second task.\n",
        "\n",
        "In the field of computer vision, we can leverage this technique to not need to train a network from scratch every time we encounter a new task. Instead, we reuse networks that are pre-trained on a massive dataset to learn a feature representation of our data. In this project, the VGGFace network is used to learn the feature representation of our dataset. This network is pre-trained on a dataset of 2.6M images of over 2.6K people. We will further explain the structure of the network in later sections.\n",
        "\n",
        "---\n",
        "\n",
        "Pan, S. J. & Yang, Q. (2010). *A Survey on Transfer Learning*. IEEE Trans. on Knowl. and Data Eng., 22, 1345--1359. DOI: 10.1109/tkde.2009.191 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgypr4okESez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras_vggface import VGGFace\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.preprocessing import image\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras.utils import np_utils\n",
        "from keras.layers import Input\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import svm\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "from tensorflow.python.keras import backend as k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISciAHRfJnKU",
        "colab_type": "text"
      },
      "source": [
        "## Prepocessing\n",
        "\n",
        "We start by reading all the pictures into a list, unifying the size of the pictures as $224\\times224$. The VGGFace model requires the images to have the same size, and the default image size is $224\\times224$. It is also possible to resize the images to other size using average pooling.\n",
        "\n",
        "As for the preprocessing procedures for each individual image, we need to adjust the pixel values of our input images, which range from 0 to 255. Now we need to subtract the mean values provided by the VGGFace model. There are two types of input format for the VGGFace model, namely *channels_first* and *channels_last*. Using *np.expand_dims()*, we transform our input images to *channels_first* format. Then we use *preprocessing_input()* from *keras_vggface* package to subtract the means by $[93.5940, 104.7624, 129.1863]$. The values $[93.5940, 104.7624, 129.1863]$ are pre-defined according to the training data used for training the VGGFace model in the first place. We can expect slightly different values for *channels_last* input images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LInO11YJiDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg_images = []\n",
        "for faces_ in images_face:\n",
        "  vgg_images_ = []\n",
        "  for face in faces_:\n",
        "    face = cv2.resize(face, (224, 224))\n",
        "    face = image.img_to_array(face)\n",
        "    face = np.expand_dims(face, axis=0)\n",
        "    face = preprocess_input(face)\n",
        "    vgg_images_.append(face)\n",
        "  vgg_images_ = np.array(vgg_images_)\n",
        "  vgg_images_ = np.rollaxis(vgg_images_, 1, 0)\n",
        "  vgg_images.append(vgg_images_)\n",
        "vgg_images = [x[0] for x in vgg_images]\n",
        "\n",
        "trainset = np.concatenate((vgg_images[0][:20], vgg_images[1][:20]))\n",
        "print(trainset.shape)\n",
        "\n",
        "testset = np.concatenate((vgg_images[0][20:], vgg_images[1][20:]))\n",
        "print(testset.shape)\n",
        "\n",
        "testCD = np.concatenate((vgg_images[2][:], vgg_images[3][:]))\n",
        "print(testCD.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkvnNxmrgfWm",
        "colab_type": "text"
      },
      "source": [
        "## Feature extraction using VGGFace\n",
        "\n",
        "Here we use the pre-trained VGGFace model for feature extraction. The\n",
        "VGGFace model is a deep convolutional neural network with 1 input layer,\n",
        "12 convolutional layers, 5 max pooling layers, 3 fully connected (FC) \n",
        "layers and  1 softmax layer . The model was pre-trained using 2.6M\n",
        "human faces.\n",
        "\n",
        "![VGGFace structure](https://i2.wp.com/sefiks.com/wp-content/uploads/2018/08/vgg-face-model.png?ssl=1)\n",
        "\n",
        "The input layer requires that the input images have the same size. The default size of the image is $224 \\times 224$.\n",
        "\n",
        "For all the convolutional layers, the convolutional filters have the same size of $3 \\times 3$, and ReLU is used as the activation function. The output of the last convolutional layer (Conv-512) gives us a feature representation of the input images with dimension (1, 7, 7, 512). We use these features for classification and identification tasks later.\n",
        "\n",
        "The 3 FC layers are also convolutional layers with a ReLU activation, where the size of the filters matches the size of the input data. Together with the softmax layer, these layers are used for class prediction, which we do not need in our case. We specify *include_top=False* to exclude these layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOafwhO0Rjne",
        "colab_type": "text"
      },
      "source": [
        "We extract the features for all our data and prepare the ground-truth labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWS1L_2rghQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vgg_features = VGGFace(include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "pred_feature_train = []\n",
        "pred_feature_test = []\n",
        "pred_feature_testCD = []\n",
        "\n",
        "# VGG features for train set\n",
        "for i in range(40):\n",
        "    x = np.asarray(trainset[i])\n",
        "    x = x.reshape((1, 224, 224, 3))\n",
        "    pred = vgg_features.predict(x)\n",
        "    pred_feature_train.append(pred)\n",
        "\n",
        "# VGG features for test set\n",
        "for i in range(20):\n",
        "    x = np.asarray(testset[i])\n",
        "    x = x.reshape((1, 224, 224, 3))\n",
        "    pred = vgg_features.predict(x)\n",
        "    pred_feature_test.append(pred)\n",
        "    \n",
        "# VGG features for persons C and D\n",
        "for i in range(20):\n",
        "    x = np.asarray(testCD[i])\n",
        "    x = x.reshape((1, 224, 224, 3))\n",
        "    pred = vgg_features.predict(x)\n",
        "    pred_feature_testCD.append(pred)\n",
        "\n",
        "feature_train = np.asarray(pred_feature_train)\n",
        "feature_train = feature_train.reshape((40, 7*7*512))\n",
        "\n",
        "feature_test = np.asarray(pred_feature_test)\n",
        "feature_test = feature_test.reshape((20, 7*7*512))\n",
        "\n",
        "feature_testCD = np.asarray(pred_feature_testCD)\n",
        "feature_testCD = feature_testCD.reshape((20, 7*7*512))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWw073QERHtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the number of classes\n",
        "num_classes = 2\n",
        "num_train = trainset.shape[0]\n",
        "labels_train = np.ones((num_train,), dtype='int32')\n",
        "\n",
        "labels_train[0:20]  = 0\n",
        "labels_train[20:40] = 1\n",
        "\n",
        "# Define the number of classes\n",
        "num_test = testset.shape[0]\n",
        "labels_test = np.ones((num_test,), dtype='int32')\n",
        "labels_test[0:10]  = 0\n",
        "labels_test[10:20] = 1\n",
        "\n",
        "labels_testCD = np.ones((num_test,), dtype='int32')\n",
        "labels_testCD[0:10]  = 0\n",
        "labels_testCD[10:20] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPSRodCqgj_x",
        "colab_type": "text"
      },
      "source": [
        "## t-SNE visualization\n",
        "\n",
        "t-SNE is widely used for high-dimensional data visualization. In high-dimensional space, the distance between two points is mapped to a normal distribution. For those data points that are closer, the probability of them being in the same cluster will be higher.\n",
        "\n",
        "From the original feature space, we obtain the reduced feature space by minimizing the sum of Kullback-Leibler (KL) divergence. The KL divergence measures the distance between probability distributions. The smaller it is, the closer two distributions are. Using gradient\n",
        "descent, we minimize the sum of KL divergence so that we end up with the \"best-fitted\" reduced feature space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwz8VFFngs8l",
        "colab_type": "text"
      },
      "source": [
        "We now use t-SNE for visualization. Our generated VGG features are easy to discriminate between groups, so we do not need to change most of the parameters. \n",
        "\n",
        "The perplexity is the most important parameter to tune. It is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Here we applied four different perplexity values, namely 5, 10, 20 and 30. It is worth mentioning that the results might vary each time you train a t-SNE model.\n",
        "\n",
        "The learning rate, which is usually within the range of [10, 1000], is set to 50. We fix the other parameters to the default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJgtRDKJgumc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perplexity_list = [5, 10, 20, 30]\n",
        "for i in perplexity_list:\n",
        "  # Define Model\n",
        "  tsne = TSNE(learning_rate=50, perplexity=i)\n",
        "  # Fit Model\n",
        "  transformed = tsne.fit_transform(feature_train)\n",
        "  # Plot 2d t-Sne\n",
        "  x_axis = transformed[:, 0]\n",
        "  y_axis = transformed[:, 1]\n",
        "\n",
        "  label_tsne = labels_train\n",
        "  \n",
        "  plt.figure()\n",
        "  palette = sns.color_palette(\"bright\", 2)\n",
        "  sns.scatterplot(x_axis, y_axis, hue=label_tsne, legend='full', palette=palette)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXY3SG9Qws3y",
        "colab_type": "text"
      },
      "source": [
        "Next, we visualize the feature representations of the 4 persons A, B, C and D using t-SNE. Based on the results of the previous section, we set the perplexity to 10. It can be seen that we obtain two clusters of features, one for persons A and C and one for persons B and D. No significant difference between persons A and B and between persons C and D can be observed. We can expect our classifiers to do a great job with very good generalization in the next section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cshu43QWwHhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_all = np.concatenate((feature_train, feature_test))\n",
        "feature_all = feature_all.reshape((60, 7*7*512))\n",
        "\n",
        "# Defining Model\n",
        "tsne = TSNE(learning_rate=50, perplexity=10)\n",
        "\n",
        "# Fitting Model\n",
        "transformed = tsne.fit_transform(feature_all)\n",
        "\n",
        "# Plotting 2d t-Sne\n",
        "x_axis = transformed[:, 0]\n",
        "y_axis = transformed[:, 1]\n",
        "\n",
        "yLabel = [2] * len(testImageC) + [3] * len(testImageD)\n",
        "yLabel = np.array(yLabel)\n",
        "label_tsne2 = np.concatenate((labels_train, yLabel))\n",
        "\n",
        "palette = sns.color_palette(\"bright\", 4)\n",
        "sns.scatterplot(x_axis, y_axis, hue=label_tsne2, legend='full', palette=palette)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5yDaqfxg5jW",
        "colab_type": "text"
      },
      "source": [
        "## Classification & Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kHj79BkMWDJ",
        "colab_type": "text"
      },
      "source": [
        "Here we use a SVM with a linear kernel as our classifier. It can be seen that all the test images of all persons are correcly classified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtGH4Flzg7Bq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_clf = svm.SVC(kernel='linear')\n",
        "svm_clf.fit(feature_train, labels_train)\n",
        "\n",
        "print('Performance on test images of persons A and B')\n",
        "pred_label_svm = svm_clf.predict(feature_test).reshape((20, 1))\n",
        "print(confusion_matrix(pred_label_svm, labels_test))\n",
        "\n",
        "print('\\nPerformance on test images of persons C and D')\n",
        "pred_labelCD_svm = svm_clf.predict(feature_testCD).reshape((20, 1))\n",
        "print(confusion_matrix(pred_labelCD_svm, labels_testCD))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wxoe6ENw69v",
        "colab_type": "text"
      },
      "source": [
        "We achieve the same performance with a 2-layer neural network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_RKS9cUxM77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp_clf2 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(12,3), random_state=1)\n",
        "mlp_clf2.fit(feature_train, labels_train)\n",
        "pred_label_mlp2 = mlp_clf2.predict(feature_test)\n",
        "\n",
        "print('Performance on test images of persons A and B')\n",
        "pred_label_mlp2 = pred_label_mlp2.reshape((20,1))\n",
        "print(confusion_matrix(pred_label_mlp2, labels_test))\n",
        "\n",
        "print('\\nPerformance on test images of persons C and D')\n",
        "pred_labelCD_mlp2 = mlp_clf2.predict(feature_testCD).reshape((20, 1))\n",
        "print(confusion_matrix(pred_labelCD_mlp2, labels_testCD))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8-eiLvshAiA",
        "colab_type": "text"
      },
      "source": [
        "Finally, KNN with $K = 5$ is applied and all the test images of persons A and B are identified correctly. Moreover, we generalize very well to the test images of persons C and D."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWHX4yANhCO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(feature_train, labels_train)\n",
        "\n",
        "print('Performance on test images of persons A and B')\n",
        "pred_label_knn = knn_clf.predict(feature_test).reshape((20, 1))\n",
        "print(confusion_matrix(pred_label_knn, labels_test))\n",
        "\n",
        "print('\\nPerformance on test images of persons C and D')\n",
        "pred_labelCD_knn = knn_clf.predict(feature_testCD).reshape((20, 1))\n",
        "print(confusion_matrix(pred_labelCD_knn, labels_testCD))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qntc1T8pmqyj",
        "colab_type": "text"
      },
      "source": [
        "# Impress the TA's"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hgiLgelnHtB",
        "colab_type": "text"
      },
      "source": [
        "## Recognizing a person in a crowd using a sliding window\n",
        "\n",
        "A frequently discussed topic in computer vision is how to find whether an object of interest is present in an image. The most straightforward approach to solving this is using a sliding window. Sliding windows play an integral role in object classification, as they allow us to localize precisely where in an image an object resides. We simply slide a window over an image and we each time try to identify whether this window contains the object or person we are looking for. Here we want to find Natalie Portman (person B) in a crowd of people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXcSVjb0nSOW",
        "colab_type": "text"
      },
      "source": [
        "First, we load the image with many people in the background and the target person, Natalie Portman, in the foreground."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dMRS4qhv4dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imutils\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blyMemViOeG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the image\n",
        "url = \"https://images.thestar.com/0nxeh0-5bN08trlMT-wG8OYdCoU=/1200x800/smart/filters:cb(2700061000)/https://www.thestar.com/content/dam/thestar/entertainment/movies/2015/08/10/natalie-portman-in-spotlight-for-tiff-fundraiser/portman.jpg\"\n",
        "\n",
        "response = requests.get(url)\n",
        "img = Image.open(BytesIO(response.content))\n",
        "img = np.array(img) \n",
        "# Convert RGB to BGR \n",
        "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "img = cv2.resize(img, (640, 640))\n",
        "cv2_imshow(img)\n",
        "(winW, winH) = (320, 320)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fiB5RKJnW36",
        "colab_type": "text"
      },
      "source": [
        "Next, we build the image pyramid function. An image pyramid is a multi-scale representation of an image. Via the image pyramid, we are able to find the target object at different scales in the image. We also need to set the minimum size of the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jODc90tzOisT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pyramid(image, scale=1.5, minSize=(30, 30)):\n",
        "\t# yield the original image\n",
        "\tyield image\n",
        "\t# keep looping over the pyramid\n",
        "\twhile True:\n",
        "\t\t# compute the new dimensions of the image and resize it\n",
        "\t\tw = int(image.shape[1] / scale)\n",
        "\t\timage = imutils.resize(image, width=w)\n",
        "\t\t# if the resized image does not meet the supplied minimum\n",
        "\t\t# size, then stop constructing the pyramid\n",
        "\t\tif image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
        "\t\t\tbreak\n",
        "\t\t# yield the next image in the pyramid\n",
        "\t\tyield image\n",
        "\n",
        "    \n",
        "def sliding_window(image, stepSize, windowSize):\n",
        "\t# slide a window across the image\n",
        "\tfor y in range(0, image.shape[0], stepSize):\n",
        "\t\tfor x in range(0, image.shape[1], stepSize):\n",
        "\t\t\t# yield the current window\n",
        "\t\t\tyield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDjqYo5SUQ46",
        "colab_type": "text"
      },
      "source": [
        "### Sliding window with PCA features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHyO_fspnbHi",
        "colab_type": "text"
      },
      "source": [
        "For the classification of Natalie Portman inside the window, we first use a two-layer neural network with PCA as features, since we obtained a good accuracy of 95% with it. We redefine the classification function here in a function since we will need to call it every time in the for loop while sliding the window. Since we will need to use the mean value and eigen faces fetched from the training images during PCA feature extraction to build the features in the test window image, we also return these.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M6WHTe4njYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classification():\n",
        "  \n",
        "  # Training data\n",
        "\n",
        "  train = aligned_faces[0][:20].copy()\n",
        "  train.extend(aligned_faces[1][:20].copy())\n",
        "\n",
        "  # PCA\n",
        "\n",
        "  mean, eigen_faces = pca(train, 15)\n",
        "  train_data = []\n",
        "  for i in range(len(train)):\n",
        "    weights = reconstruct(train[i], mean, eigen_faces)\n",
        "    if i < 20:\n",
        "      train_data.append((weights, 0))\n",
        "    else:\n",
        "      train_data.append((weights, 1))\n",
        "  \n",
        "  # Train classifier\n",
        "\n",
        "  X = [x[0] for x in train_data]\n",
        "  y = [x[1] for x in train_data]\n",
        "\n",
        "  clf = MLPClassifier(solver='lbfgs', alpha=1e-3, hidden_layer_sizes=(8,6), random_state=1)\n",
        "  clf.fit(X, y)\n",
        "\n",
        "  return clf, mean, eigen_faces"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCPSP4rmnr9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construct the model\n",
        "mlp, mean_C, eigen_faces_C = classification()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj2BY75vnwYu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "After applying classification to every window, we keep those windows that are classified as containing person B (label = 1), which is Natalie Portman. By observing the final result, every printed image includes Natalie Portman; this means that the sliding window works well for detecting the target person in a group of various people."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZjaAMy3nyc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(winW, winH) = (256, 256)  # size for the sliding window\n",
        "image_pos1 = []\n",
        "\n",
        "# loop over the image pyramid\n",
        "for resized in pyramid(img, scale=1.5):\n",
        "  # loop over the sliding window for each layer of the pyramid\n",
        "  for (x, y, window) in sliding_window(resized, stepSize=20, windowSize=(winW, winH)):\n",
        "    # if the window does not meet our desired window size, ignore it\n",
        "    if window.shape[0] != winH or window.shape[1] != winW:\n",
        "      continue\n",
        "    window = cv2.resize(window, (256, 256))\n",
        "    \n",
        "    weights = reconstruct(window, mean_C, eigen_faces_C)\n",
        "    \n",
        "    classif = mlp.predict([weights])\n",
        "    if classif == 1:\n",
        "      image_pos1.append(window)\n",
        "  \n",
        "for frame in image_pos1:\n",
        "  cv2_imshow(frame)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FAWWIl_oAVM",
        "colab_type": "text"
      },
      "source": [
        "### Sliding window with VGG features\n",
        "\n",
        "Similarly, we apply the same procedure for features generated using VGGFace. Since we obtain perfect classification results from the KNN classifier, we apply the KNN classifier (with $K = 5$) again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwYNNttPOlaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Consruct the model\n",
        "knn_clf1 = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf1.fit(feature_train, labels_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTmIpJ9SOrr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(winW, winH) = (256, 256)  # size for the sliding window\n",
        "image_pos2 = []\n",
        "\n",
        "# loop over the image pyramid\n",
        "for resized in pyramid(img, scale=1.5):\n",
        "  # loop over the sliding window for each layer of the pyramid\n",
        "  for (x, y, window) in sliding_window(resized, stepSize=20, windowSize=(winW, winH)):\n",
        "    # if the window does not meet our desired window size, ignore it\n",
        "    if window.shape[0] != winH or window.shape[1] != winW:\n",
        "      continue\n",
        "    window = cv2.resize(window, (224, 224))\n",
        "\n",
        "    x = image.img_to_array(window)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    x = x.reshape((1, 224, 224,3))\n",
        "\n",
        "    vggf =  vgg_features.predict(x)\n",
        "    vggf = vggf.reshape((1, 7*7*512))\n",
        "    classif = knn_clf1.predict(vggf)\n",
        "    if classif == 1:\n",
        "      image_pos2.append(window)\n",
        "  \n",
        "for frame in image_pos2:\n",
        "  cv2_imshow(frame)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TfJ2zWImxkc",
        "colab_type": "text"
      },
      "source": [
        "# Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr9kXgRm6SIj",
        "colab_type": "text"
      },
      "source": [
        "Troughout this assignment we tested several different feature representation methods for the task of face classification. Each feature representation has its strenghts and weaknesses. However, by using the representations as input for different classifiers, it became apparent that some give better overall results than others.\n",
        "\n",
        "**Overview of results**\n",
        "\n",
        "\n",
        "|                  | **SIFT** | **PCA**  | **Transfer learning** |\n",
        "| ---------------- | -------- | -------- | --------------------- |\n",
        "| **Person A & B** | 85%      | 85%      | 100%                  |\n",
        "| **Person C & D** | 70%      | 65%      | 100%                  |\n",
        "\n",
        "Table 1: Accuracy obtained by the linear support vector machine classifier on the test set for each feature representation.\n",
        "\n",
        "\n",
        "\n",
        "|                  | **SIFT** | **PCA**  | **Transfer learning** |\n",
        "| ---------------- | -------- | -------- | --------------------- |\n",
        "| **Person A & B** | 85%      | 100%     | 100%                  |\n",
        "| **Person C & D** | 75%      | 75%      | 100%                  |\n",
        "\n",
        "Table 2: Accuracy obtained by the multi-layer perceptron classifier on the test set for each feature representation.\n",
        "\n",
        "|                  | **SIFT** | **PCA**  | **Transfer learning** |\n",
        "| ---------------- | -------- | -------- | --------------------- |\n",
        "| **Person A & B** | 85%      | 75%      | 100%                  |\n",
        "| **Person C & D** | 70%      | 60%      | 100%                  |\n",
        "\n",
        "Table 3: Accuracy obtained by the K nearest neighbours classifier on the test set for each feature representation. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vipMPVLoEzE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**SIFT.** \n",
        "\n",
        "The advantage of SIFT features, and the reason for which they were designed,  is their invariance properties with respect to rotation, scaling and affine transformations. While the images of the same person in our dataset are quite heterogeneous, there are still some notable differences in appearance, illumination, background and pose. The strength of the invariance properties of SIFT features is shown in the fact that we could achieve very good classification and identification performance on the test images of persons A and B. However, SIFT does struggle with occlusions and large shadows, this can be seen in the test image of person A where he is holding an object in front of his face, this image is always wrongly classified. \n",
        "\n",
        "It is also not necessary to have a large dataset for feature extraction; SIFT is a general and readily applicable technique that can immediately be used on individual images.\n",
        "\n",
        "This final point is also a disadvantage. Because SIFT is not data-driven, the feature space that it produces cannot be optimized for a specific task or a specific set of instances. This can be seen in the significatntly worse classification accuracy for persons C and D. The features of these persons are not constructed in the context of the appearances of persons A and B, making it difficult to immediately match them and assign person C to A and D to B. The features are constructed in isolation and appear to differ significantly. Also, SIFT is quite computationally expensive.\n",
        "\n",
        "**PCA.** \n",
        "\n",
        "PCA is very efficient in the sense that we can achieve a very strong dimensionality reduction, while keeping the discriminative power to classify images reasonably well. We also have flexibility in the number of principal components to retain. It slightly improves classification performance over the SIFT features, and importantly, it does so with a large reduction in the number of features.\n",
        "\n",
        "The disadvantage is that for PCA to really work well, training and test images need to have a very coherent appearance. We perform an alignment step to reduce the variance between the images, but differences in pose, clothes, background... still significantly decrease performance. Also, the training set we use does not allow us to push the limits of the performance we can achieve, as we would need more training images to be able to increase the useful number of principal components. Using a large dataset of specifically tailored images for PCA would allow us to really assess the quality of this technique, which could be an extension when we had more time. While this reduces the robustness and usability of this technique, it is still interesting that we can achieve quite good results on a difficult task for PCA with very few features.\n",
        "\n",
        "**Transfer learning.** \n",
        "\n",
        "It is clear that transfer learning is by far the best approach we have used in this project. It performs perfectly in all ways. This is because we can use an extensive neural network that has the possibility to very flexibly learn all kinds of latent characteristics that are present in the images. This results in a projection of the images to a very discriminative, optimized feature representation for the task of person identification. A disadvantage is that we need an enormous amount of data to train the network, but transfer learning alleviates this issue by leveraging a pretrained network where this has already been done.\n",
        "\n",
        "**General conclusions.** \n",
        "\n",
        "As a general assessment of the performance of the difference methods, the most significant difference between handcrafted and data-driven features is that handcrafted features are constructed in isolation and only afterwards used to distinguish between the images. SIFT features are generally designed to be as robust and discriminative as possible, but they arent explicitly optimized for a particular set of instances or for a particular task during construction. This makes it difficult for them to generalize well, as the projection of images to feature space is not learned in the context of relations that exist between images.\n",
        "\n",
        "However, the data-driven methods perform projections of images to a feature space that is explicitly learned to be as discriminative as possible for a particular set of instances. This means that data-driven methods are more performant because they perform the tasks of classification and identification in a space that is flexibly learned to be optimal in some way to distinguish between a particular set of images.\n",
        "\n",
        "Nevertheless, transfer learning outperforms PCA. Transfer learning performs better than PCA because PCA learns a feature space that is optimized with respect to the statistical measure of variance, whereas transfer learning projects images to a space that is optimized with respect to arbitrary latent characteristics of the images under consideration. This makes transfer learning more flexible than PCA because it is not restricted to only dealing with variance and it has a lot of freedom to learn something optimal for the task at hand. This is a major advantage of deep learning and it is the reason why it has been so popular in recent years.\n",
        "\n",
        "**Additional remarks and possible improvements**\n",
        "\n",
        "The previous section provides a detailed discussion on each of the feature representations and their relative strengths and weaknesses. In this final section we go a bit further and try to explore some ways in which our feature representation and therefore the classifier could be further improved. We also look at some possible pitfalls when using this system as a security face identification system.\n",
        " \n",
        "Due to the limited time-frame in which this assignment had to be completed, we did not get to implement all our ideas to improve the system. To improve the performance of the SIFT feature algorithm it would be interesting to implement SURF (Bay et al., 2006) or ORB (Rublee et al., 2011). To add color invariance to the SIFT features we could also implement CSIFT (Abdel-Hakim & Farag, 2006).\n",
        "\n",
        "Various improvements are still possible/necessary to our most performant feature representation: the transfer learning features, especially to make it function properly as a company security system. At the moment, the dataset is very small and only contains two people. Therefore, the network will predict either person A or person B when presented with a picture. In a real-world setting this is undesirable, we need a third class with faces and even objects that are not known. An efficient way of training this third class is to use our moving window implementation. By applying our moving window algorithm to different pictures that are very crowded, we will have at once a large number of false positives on which the network can be retrained. \n",
        "\n",
        "Another necessary step is to make the network more robust to adversarial attacks. One way of doing this has already been mentioned, namely creating a larger dataset. However, it has been shown that this alone is not enough. The reason for this is that, even with a large dataset, the machine learning models are only trained on a small subset of the whole feature space. The high-dimensional feature space, especially in image analysis, is so sparse that most of the training data is located only in a small region; this region is referred to as the manifold. Therefore, the network can be fooled by creating images that fall outside of the manifold. This can be done by something as simple as wearing glasses with a specific pattern (Sharif et al., 2016).  Knowing this, specific datasets have to be used to test the robustness of the network and improve it. An example of such a dataset is Damagenet which was created for the ImageNet dataset and able to fool many models trained on the dataset (Chen et al., 2019)\n",
        "\n",
        "**Bibliography**\n",
        "\n",
        "Abdel-Hakim, Alaa & Farag, Aly. (2006). CSIFT: A SIFT descriptor with color invariant characteristics. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 2. 1978 - 1983. 10.1109/CVPR.2006.95. \n",
        "\n",
        "Bay, H., Tuytelaars, T., & Van Gool, L. (2006, May). Surf: Speeded up robust features. In \n",
        "European conference on computer vision (pp. 404-417). Springer, Berlin, Heidelberg.\n",
        "\n",
        "Chen, S., Huang, X., He, Z., & Sun, C. (2019). DAmageNet: A Universal Adversarial Dataset\n",
        ". arXiv:1912.07160\n",
        "\n",
        "Rublee, E., Rabaud, V., Konolige, K., & Bradski, G. (2011, November). ORB: An efficient alternative to SIFT or SURF. In 2011 International conference on computer vision (pp. 2564-2571). \n",
        "\n",
        "Sharif, M., Bhagavatula, S., Bauer, L., & Reiter, M. K. (2016, October). Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 2016 acm sigsac conference on computer and communications security (pp. 1528-1540).\n",
        "\n"
      ]
    }
  ]
}